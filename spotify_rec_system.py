# -*- coding: utf-8 -*-
"""Spotify_Rec_System.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ik8VQJr92FLOMTHU53gAwuXqdCNvv21s

# Music Recommendation System with Natural Language Processing

## Import Necessary Libraries
"""

# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import NearestNeighbors
from sklearn.decomposition import TruncatedSVD
from wordcloud import WordCloud

# Ensure necessary NLTK resources are downloaded
nltk.download('punkt')
nltk.download('stopwords')

"""## Load and Explore Dataset"""

file_path = "/content/tcc_ceds_music.csv"  # Path to the uploaded file in Colab
df = pd.read_csv(file_path)

# Display first few rows and dataset info
display(df.head())
display(df.info())

"""# EDA

We will begin by examining the dataset, by viewing the head to get a sense of the column layouts and their values. We will then use the info command to check the data types of each variable as well as the non-null count.

As can be seen, we have 30 columns and 28,372 rows. There are no missing values in any column which makes our analysis easier and we do not have to do any imputation. Most of the variables are float64 data types with the exception being artist_name, track_name, genre, lyrics, and age which are objects. release_date and len are int64 variables which are still numeric. This means we have 25 numeric variables and 5 categorical ones.
"""

# Drop unnecessary columns
df.drop(columns=['Unnamed: 0'], inplace=True)

"""We will now look at the Top 5 most common values for each categorical variable. We can see Johnny Cash is the most occuring artist, sadness is the most popular topic, and pop is the most popular genre."""

# For categorical columns
categorical_cols = df.select_dtypes(include=['object']).columns
categorical_cols = categorical_cols[categorical_cols != 'lyrics']
for col in categorical_cols:
    print(f'{col} unique values: {df[col].nunique()}')
    print(df[col].value_counts().head())  # Show top 5 most frequent categories for each

# Data appears to be mostly numerical types with the majority in float64 types. There area few categorical text base
# Genre, Lyrics, and Topic.
# For numerical columns
numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns
print(df[numerical_cols].describe())



"""We will now look at histograms of our 25 numerical variables to get a sense of how they are distributed. These histograms display count values so we can see if certain patterns emerge and this will help us find correlations between variables."""

import matplotlib.pyplot as plt
import seaborn as sns

# Histogram for numerical columbns
df[numerical_cols].hist(bins=20, figsize=(15, 10))
plt.tight_layout()
plt.show()

"""A few key patterns emerge from these histograms. Length is right tailed with a mean of about 50, and release date appears to have more songs from newer years. Danceability, valence, and energy have means of about 0.5 and have even tails on both sides. The bulk of the variables that contain emotional connotations of songs such as world/life, romantic, and sadness have very high means at just above 0, and have few values outside of this range.

We will now look at barplots of the counts of each genre and song topic. Both genre and topic are expected to categorize similar songs together as differences between musical genres are quite large and users are predicted to generally like similar genre songs.
"""

plt.figure(figsize=(10, 6))
sns.countplot(data=df, x='genre', order=df['genre'].value_counts().index)
plt.title('Genre Distribution')
plt.xticks(rotation=90)
plt.show()

# Plot for other categorical columns if needed (e.g., artist_name, topic)
sns.countplot(data=df, x='topic', order=df['topic'].value_counts().index)
plt.title('Topic Distribution')
plt.xticks(rotation=90)
plt.show()

# Boxplot for numeric features based on genre
plt.figure(figsize=(6, 3))
plt.subplot(2,2,1)
sns.boxplot(data=df, x='genre', y='sadness') # Example: sadness by genre
plt.xticks(rotation=90)
plt.show()

# Boxplot for numeric features based on genre
plt.figure(figsize=(6, 3))
plt.subplot(2,2,2)
sns.boxplot(data=df, x='genre', y='violence') # Example: violence by genre
plt.xticks(rotation=90)
plt.show()

# Boxplot for numeric features based on genre
plt.figure(figsize=(6, 3))
plt.subplot(2,2,3)
sns.boxplot(data=df, x='genre', y='world/life')
plt.xticks(rotation=90)
plt.show()

"""As can be seen, pop is the top category with a difference of about 1500 from the second place country. Blues, Jazz, and Rock have similar counts and reggae and hip-hop have much less. In terms of topic there is a big difference in the values of the top 4 topics and the bottom 4. The top 4: sadness, violence, world/life, and obscene have values around 4800-6000 and the bottom 4 are all below 2500. We can generally expect to see these top genres and topics being recommended more by our NLP algorithm.

We will now look at a correlation matrix of all numeric values in our dataset. We will look for variables for strong positive and negative correlations as these relationships will be impactful in interpreting the results of our song interpreter.
"""

# Correlation matrix
plt.figure(figsize=(15, 10))
corr = df[numerical_cols].corr()
sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)
plt.title('Correlation Heatmap')
plt.show()

"""We can see that the largest Positive Correlations are between: Len and Obscene, Loudness and Energy, and Danceability and Valence We can see that the largest Negative Correlations are between: Energy and Acousticness, Age and Loudness, and Loudness and Acousticness

Intuitively many of these make sense, loud songs are more likely to have more energy and acousticness would lead to quiet songs which would have a negative correlation with loudness. An interesting unexpected correlation is that longer songs seem more likely to be more obscene and also that the valence (positivity or negativity) of a song is correlated with its danceability.

We will now look at scatterplots with a best fit linear regression line of the top 3 positive and negative relationships based on our correlation matrix. We want to visualize how strong these relationships are as these will provide good insight into what factors we can expect to be most influential in our algorithm.
"""

# Define the pairs for plotting
positive_pairs = [('len', 'obscene'), ('loudness', 'energy'), ('energy', 'release_date')]
negative_pairs = [('energy', 'acousticness'), ('age', 'loudness'), ('loudness', 'acousticness')]

# Create a figure and axis for each plot
fig, axs = plt.subplots(2, 3, figsize=(15, 10))  # 2 rows, 3 columns for 6 plots
fig.suptitle('Scatter Plots Showing Correlation Between Different Features', fontsize=16)

# Plot positive correlations with line of best fit
for i, (x_col, y_col) in enumerate(positive_pairs):
    sns.regplot(x=x_col, y=y_col, data=df, ax=axs[0, i], scatter_kws={'s': 2}, line_kws={'color': 'blue'})
    axs[0, i].set_title(f'{x_col} vs {y_col} (Positive Correlation)')
    axs[0, i].set_xlabel(x_col)
    axs[0, i].set_ylabel(y_col)

# Plot negative correlations with line of best fit
for i, (x_col, y_col) in enumerate(negative_pairs):
    sns.regplot(x=x_col, y=y_col, data=df, ax=axs[1, i], scatter_kws={'s': 2}, line_kws={'color': 'red'})
    axs[1, i].set_title(f'{x_col} vs {y_col} (Negative Correlation)')
    axs[1, i].set_xlabel(x_col)
    axs[1, i].set_ylabel(y_col)

# Adjust the layout
plt.tight_layout()
plt.subplots_adjust(top=0.9)  # Adjust title spacing

# Show the plot
plt.show()

"""We can immediately see that loudness and energy has a very clear relationship and the regression line indicates a strong positive correlation. The negative relationships have very strong trend lines and pattern of points appear to support their accuracy. Loudness vs. Acousticness has a very strong negative correlation which makes sense as loudness, energy, and acousticness all are very closely related. (Len and Obscene) and (Energy Release_Date) have strong positive relationships but the pattern isn't quite as apparent without the best fit line.

We will now plot and examine a word cloud before doing our NLP process so we can visualize the most popular words in each song's lyrics. We expect songs with similar vocabularies to be recommened together.
"""

# Join all lyrics and create a word cloud
all_lyrics = ' '.join(df['lyrics'])
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_lyrics)

# Display the wordcloud
plt.figure(figsize=(10, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

"""We can see based on the word cloud that Come, Know, Cause, Time and Want have the largest sizes respectively. The word cloud displays in proportion to their frequencies of occuring so we can visualize which words are the most common. This is important because a big portion of NLP analysis is looking at elements like the Cosine Similarity Score to see which songs have the most words in common. We will now partition each word, remove stop words and plot a barplot of the most frequently occuring words."""

from collections import Counter
stop_words = set(stopwords.words('english'))

# Tokenizing and removing stopwords
tokens = [word for word in all_lyrics.split() if word.lower() not in stop_words]

# Get the most common words
common_words = Counter(tokens).most_common(20)
# Example common_words list

# Convert the list of tuples into a DataFrame
df_common_words = pd.DataFrame(common_words, columns=['Word', 'Count'])

# Sort by count in descending order for better visualization
df_common_words = df_common_words.sort_values(by='Count', ascending=False)

# Create the bar plot using seaborn
plt.figure(figsize=(10, 6))
sns.barplot(x='Count', y='Word', data=df_common_words, palette='viridis')

# Set plot labels and title
plt.title('Word Frequency Bar Plot')
plt.xlabel('Count')
plt.ylabel('Word')

# Display the plot
plt.show()

"""We can see that the words know, like, time, and come have by far the most occurences. We can see visualizing inspecting the words that these all seem like very common words musicians use in pop and country music in particular as well as music in general.

We will now perform Principle Component Analysis to examine which variables have the highest loadings on each principle component axis and how much each variable contributes to the cumulative variance.
"""

import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# Select numerical columns
numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns
numerical_data = df[numerical_cols]

# Standardize the data
scaler = StandardScaler()
numerical_data_scaled = scaler.fit_transform(numerical_data)

# Apply PCA
pca = PCA()
pca.fit(numerical_data_scaled)

# Explained variance for each component
explained_variance = pca.explained_variance_ratio_

# Create a bar plot for explained variance
plt.figure(figsize=(10, 6))
plt.bar(range(1, len(explained_variance) + 1), explained_variance, color='skyblue')
plt.title('Explained Variance by Each Principal Component')
plt.xlabel('Principal Component')
plt.ylabel('Explained Variance')
plt.xticks(range(1, len(explained_variance) + 1))
plt.show()

# Cumulative explained variance (optional)
cumulative_explained_variance = np.cumsum(explained_variance)

# Create a bar plot for cumulative explained variance
plt.figure(figsize=(10, 6))
plt.bar(range(1, len(cumulative_explained_variance) + 1), cumulative_explained_variance, color='skyblue')
plt.title('Cumulative Explained Variance by Principal Components')
plt.xlabel('Number of Principal Components')
plt.ylabel('Cumulative Explained Variance')
plt.xticks(range(1, len(cumulative_explained_variance) + 1))
plt.show()

import pandas as pd
import numpy as np
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# Select numerical columns
numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns
numerical_data = df[numerical_cols]

# Standardize the data
scaler = StandardScaler()
numerical_data_scaled = scaler.fit_transform(numerical_data)

# Apply PCA
pca = PCA()
pca.fit(numerical_data_scaled)

# Extract the loadings (components) for the first 3 principal components
components_df = pd.DataFrame(pca.components_, columns=numerical_cols)

# Print the first 3 components
print("First 3 Principal Components:")
for i in range(3):
    print(f"Principal Component {i+1}:")
    top_loadings = components_df.iloc[i].abs().nlargest(6).index
    for feature in top_loadings:
        print(f"{feature}: {components_df.iloc[i][feature]:.4f}")
    print("\n")

"""We can see firstly based on the first plot that the first principle contributes by far the most to the dataset. PC1 has an explained variance of about 0.16 compared to just 0.08 for PC2. It also seems that PC3 - PC18 each contribute a similar amount of explained variance and linearly increase the total proportion explained. This indicates that to reach an explained cumulative variance of 0.9 we would still need about 18 PC components to do so.

In the table above, we calculated the 5 highest absolute value loadings for the first 3 PC components and also displayed the variables alongside. We can see that for PC1 the first four variables energy, age, release_date, and acousticness have very similar loading scores and there is a drop off of about 0.13 to the 5th greatest loading score variable, len. Valenece and Danceability have the highest loadings for PC2 and Sadness has by far the greatest loading for PC3. These relationships between PC loadings are similar in many ways to some of the relationships in the correlation matrix above and indicate significant relationships between several predictor variables.
"""

import matplotlib.pyplot as plt
import seaborn as sns
# Histogram for numerical columns
df[numerical_cols].hist(bins=20, figsize=(15, 10))
plt.tight_layout()
plt.show()

plt.figure(figsize=(10, 6))
sns.countplot(data=df, x='genre', order=df['genre'].value_counts().index)
plt.title('Genre Distribution')
plt.xticks(rotation=90)
plt.show()
# Plot for other categorical columns if needed (e.g., artist_name, topic)
sns.countplot(data=df, x='topic', order=df['topic'].value_counts().index)
plt.title('Topic Distribution')
plt.xticks(rotation=90)
plt.show()

"""## Recommender System

To give users generalized reconmendations of the most popular and most relevant songs, we will explore the world of recommender systems. We want to start off with a generalized recommender system that gives users the option to input a song in the Spotify music system and provides generalized reconmendations of the most similar songs to the given input. This recommender system follows the concept of content based methods in recommender systems, where we recommend new music songs to users based on the similarity of songs that the user has liked or previewed before.
"""

# Step 1: We import necessary libraries
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD
from sklearn.neighbors import NearestNeighbors

# Step 2: We load the dataset
file_path = "/content/tcc_ceds_music.csv"  # Path to the uploaded file in Colab
df = pd.read_csv(file_path)

# Step 3: Preprocessing for our system

if 'Unnamed: 0' in df.columns:
    df.drop(columns=['Unnamed: 0'], inplace=True)

numeric_features = ['danceability', 'energy', 'acousticness', 'valence', 'loudness', 'instrumentalness', 'age']
X_numeric = df[numeric_features]

scaler = StandardScaler()
X_numeric_scaled = scaler.fit_transform(X_numeric)

df['processed_lyrics'] = df['lyrics'].apply(
    lambda x: ' '.join([word.lower() for word in str(x).split() if word.isalpha()])
)

vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')
X_text = vectorizer.fit_transform(df['processed_lyrics'])

svd = TruncatedSVD(n_components=100, random_state=42)
X_text_reduced = svd.fit_transform(X_text)

# Step 4: We combine Numerical and Text Features
X_combined = np.hstack((X_numeric_scaled, X_text_reduced))

# Step 5: We build the Recommendation System
knn = NearestNeighbors(n_neighbors=11, metric='cosine')  # 11 because input song will be included
knn.fit(X_combined)


def recommend_songs(song_name, df, knn_model, combined_features):

    if song_name not in df['track_name'].values:
        print(f"Error: The song '{song_name}' does not exist in the dataset.")
        return None

    song_index = df[df['track_name'] == song_name].index[0]

    song_features = combined_features[song_index].reshape(1, -1)

    distances, indices = knn_model.kneighbors(song_features)

    similar_song_indices = indices.flatten()[1:]

    return df.iloc[similar_song_indices]

# Step 6: We test our Recommendation System
input_song = "i believe"
recommendations = recommend_songs(input_song, df, knn, X_combined)

if recommendations is not None:
    print(recommendations[['track_name', 'artist_name', 'danceability', 'energy', 'acousticness', 'valence']])

"""In order to implement the recommender system, we first perform numerical feature scaling in order to obtain various TF-IDF values. Next, we reduce dimensionality by applying Singular Value Decomposition before finally building the recommender system. This ensures we have a working recommender system where we are able to input a song name and get the top ten most recommended most similar songs.

# Recomender system, Using NLP

We have successfully built a recommender system using simpler content-based methods. Now, we want to see how incorporating NLP and sentiment analysis will enhance our recommendations.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import NearestNeighbors
from wordcloud import WordCloud
from textblob import TextBlob
import re
import string

# Step 1: Load the dataset
file_path = "/content/tcc_ceds_music.csv"  # Path to the uploaded file in Colab
df = pd.read_csv(file_path)
df.drop(columns=['Unnamed: 0'], inplace=True)

# Create unique song identifier: artist - track
df['song_id'] = df['artist_name'] + " - " + df['track_name']

# Step 2: Clean the lyrics
def clean_lyrics(text):
    text = str(text).lower()
    text = re.sub(f"[{re.escape(string.punctuation)}]", "", text)
    text = re.sub(r'\d+', '', text)
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

df['clean_lyrics'] = df['lyrics'].apply(clean_lyrics)

# Step 3: Sentiment analysis
df['sentiment'] = df['clean_lyrics'].apply(lambda x: TextBlob(x).sentiment.polarity)

# Show WordCloud for positive/negative lyrics
positive_lyrics = ' '.join(df[df['sentiment'] > 0.4]['clean_lyrics'])
negative_lyrics = ' '.join(df[df['sentiment'] < -0.4]['clean_lyrics'])

fig, ax = plt.subplots(1, 2, figsize=(16, 6))
wordcloud_pos = WordCloud(width=800, height=400, background_color='white').generate(positive_lyrics)
wordcloud_neg = WordCloud(width=800, height=400, background_color='black', colormap='Reds').generate(negative_lyrics)

ax[0].imshow(wordcloud_pos, interpolation='bilinear')
ax[0].axis('off')
ax[0].set_title("Positive Lyrics WordCloud")

ax[1].imshow(wordcloud_neg, interpolation='bilinear')
ax[1].axis('off')
ax[1].set_title("Negative Lyrics WordCloud")

plt.tight_layout()
plt.show()

# Step 4: TF-IDF + SVD
vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')
X_text = vectorizer.fit_transform(df['clean_lyrics'])

svd = TruncatedSVD(n_components=100, random_state=42)
X_text_reduced = svd.fit_transform(X_text)

# Step 5: Scale numeric features (including sentiment)
numeric_features = ['danceability', 'energy', 'acousticness', 'valence',
                    'loudness', 'instrumentalness', 'age', 'sentiment']
X_numeric = df[numeric_features]

scaler = StandardScaler()
X_numeric_scaled = scaler.fit_transform(X_numeric)

# Step 6: Combine all features
X_combined = np.hstack((X_numeric_scaled, X_text_reduced))

# Step 7: Build the KNN model
knn = NearestNeighbors(n_neighbors=11, metric='cosine')
knn.fit(X_combined)

# Step 8: Song recommendation function using 'song_id'
def recommend_songs(song_id, df, knn_model, feature_matrix):
    if song_id not in df['song_id'].values:
        print(f"'{song_id}' not found in dataset.")
        return None

    song_index = df[df['song_id'] == song_id].index[0]
    song_vector = feature_matrix[song_index].reshape(1, -1)

    distances, indices = knn_model.kneighbors(song_vector)
    similar_indices = indices.flatten()[1:]  # exclude the input song itself

    recommendations = df.iloc[similar_indices].copy()
    return recommendations

# Step 9: Test the recommender
input_song_id = "nujabes - luv (sic.) pt3 (feat. shing02)"  # Replace with a valid artist - track from your dataset
recommendations = recommend_songs(input_song_id, df, knn, X_combined)

if recommendations is not None:
    print("Top 10 similar songs to:", input_song_id)
    display(recommendations[['song_id', 'danceability', 'energy', 'valence', 'sentiment']])

    # Step 10: Visualize sentiment of recommended songs
    plt.figure(figsize=(10, 5))
    sns.barplot(x='song_id', y='sentiment', data=recommendations)
    plt.xticks(rotation=45, ha='right')
    plt.title('Sentiment Scores of Recommended Songs')
    plt.ylabel('Polarity (-1 to 1)')
    plt.xlabel('Song (Artist - Track)')
    plt.tight_layout()
    plt.show()

"""**Each song lyric is assigned a sentiment polarity score:**  
-1 = very negative.
+1 = very positive.

Extract top 5,000 important words per lyric using TF-IDF
Reduce dimensionality using Truncated SVD to 100 components

**Scaled numeric features**: danceability, energy, valence, etc.  
Combined with lyric features into one unified vector (X_combined)

**Model**: KNN using cosine similarity
Finds the 10 most similar songs based on track_name, artist_name, danceability, energy, valence, sentiment.

**Overall WordCloud**: Shows most common words in all lyrics.

**Positive vs. Negative WordClouds**: Contrast in word usage by sentiment

**Outputs top 10 similar tracks with metadata**:track_name, artist_name, danceability, energy, valence, sentiment.
Bar chart of sentiment scores for recommended songs helps evaluate the emotional similarity to the input song

"""

